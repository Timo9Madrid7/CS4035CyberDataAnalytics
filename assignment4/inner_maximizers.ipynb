{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "\"\"\"\n",
    "Python module for implementing inner maximizers for robust adversarial training\n",
    "(Table I in the paper)\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from utils.utils import or_float_tensors, xor_float_tensors, clip_tensor\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# helper function\n",
    "def round_x(x, alpha=0.5):\n",
    "    \"\"\"\n",
    "    rounds x by thresholding it according to alpha which can be a scalar or vector\n",
    "    :param x:\n",
    "    :param alpha: threshold parameter\n",
    "    :return: a float tensor of 0s and 1s.\n",
    "    \"\"\"\n",
    "    return (x > alpha).float()\n",
    "\n",
    "\n",
    "def get_x0(x, is_sample=False):\n",
    "    \"\"\"\n",
    "    Helper function to randomly initialize the the inner maximizer algos\n",
    "    randomize such that the functionality is preserved.\n",
    "    Functionality is preserved by maintaining the features present in x\n",
    "    :param x: training sample\n",
    "    :param is_sample: flag to sample randomly from feasible area or return just x\n",
    "    :return: randomly sampled feasible version of x\n",
    "    \"\"\"\n",
    "    if is_sample:\n",
    "        rand_x = round_x(torch.rand(x.size()))\n",
    "        if x.is_cuda:\n",
    "            rand_x = rand_x.cuda()\n",
    "        return or_float_tensors(x, rand_x)\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "\n",
    "def dfgsm_k(x,\n",
    "            y,\n",
    "            model,\n",
    "            loss_fct,\n",
    "            k=25,\n",
    "            epsilon=0.02,\n",
    "            alpha=0.5,\n",
    "            is_report_loss_diff=False,\n",
    "            use_sample=False):\n",
    "    \"\"\"\n",
    "    FGSM^k with deterministic rounding\n",
    "    :param y:\n",
    "    :param x: (tensor) feature vector\n",
    "    :param model: nn model\n",
    "    :param loss_fct: loss function\n",
    "    :param k: num of steps\n",
    "    :param epsilon: update value in each direction\n",
    "    :param alpha:\n",
    "    :param is_report_loss_diff:\n",
    "    :param use_sample:\n",
    "    :return: the adversarial version of x according to dfgsm_k (tensor)\n",
    "    \"\"\"\n",
    "    # some book-keeping\n",
    "    if next(model.parameters()).is_cuda:\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "    y = Variable(y)\n",
    "\n",
    "    # compute natural loss\n",
    "    loss_natural = loss_fct(model(Variable(x)), y).data\n",
    "\n",
    "    # initialize starting point\n",
    "    x_next = get_x0(x, use_sample)\n",
    "\n",
    "    # multi-step\n",
    "    for t in range(k):\n",
    "        # forward pass\n",
    "        x_var = Variable(x_next, requires_grad=True)\n",
    "        y_model = model(x_var)\n",
    "        loss = loss_fct(y_model, y)\n",
    "\n",
    "        # compute gradient\n",
    "        grad_vars = torch.autograd.grad(loss.mean(), x_var)\n",
    "\n",
    "        # find the next sample\n",
    "        x_next = x_next + epsilon * torch.sign(grad_vars[0].data)\n",
    "\n",
    "        # projection\n",
    "        x_next = clip_tensor(x_next)\n",
    "\n",
    "    # rounding step\n",
    "    x_next = round_x(x_next, alpha=alpha)\n",
    "\n",
    "    # feasible projection\n",
    "    x_next = or_float_tensors(x_next, x)\n",
    "\n",
    "    # compute adversarial loss\n",
    "    loss_adv = loss_fct(model(Variable(x_next)), y).data\n",
    "\n",
    "    if is_report_loss_diff:\n",
    "        print(\"Natural loss (%.4f) vs Adversarial loss (%.4f), Difference: (%.4f)\" %\n",
    "              (loss_natural.mean(), loss_adv.mean(), loss_adv.mean() - loss_natural.mean()))\n",
    "\n",
    "    replace_flag = (loss_adv < loss_natural).unsqueeze(1).expand_as(x_next)\n",
    "    x_next[replace_flag] = x[replace_flag]\n",
    "\n",
    "    if x_next.is_cuda:\n",
    "        x_next = x_next.cpu()\n",
    "\n",
    "    return x_next\n",
    "\n",
    "\n",
    "def rfgsm_k(x, y, model, loss_fct, k=25, epsilon=0.02, is_report_loss_diff=False, use_sample=False):\n",
    "    \"\"\"\n",
    "    FGSM^k with randomized rounding\n",
    "    :param x: (tensor) feature vector\n",
    "    :param y:\n",
    "    :param model: nn model\n",
    "    :param loss_fct: loss function\n",
    "    :param k: num of steps\n",
    "    :param epsilon: update value in each direction\n",
    "    :param is_report_loss_diff:\n",
    "    :param use_sample:\n",
    "    :return: the adversarial version of x according to rfgsm_k (tensor)\n",
    "    \"\"\"\n",
    "    # some book-keeping\n",
    "    if next(model.parameters()).is_cuda:\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "    y = Variable(y)\n",
    "\n",
    "    # compute natural loss\n",
    "    loss_natural = loss_fct(model(Variable(x)), y).data\n",
    "\n",
    "    # initialize starting point\n",
    "    x_next = get_x0(x, use_sample)\n",
    "\n",
    "    # multi-step with gradients\n",
    "    for t in range(k):\n",
    "        # forward pass\n",
    "        x_var = Variable(x_next, requires_grad=True)\n",
    "        y_model = model(x_var)\n",
    "        loss = loss_fct(y_model, y)\n",
    "\n",
    "        # compute gradient\n",
    "        grad_vars = torch.autograd.grad(loss.mean(), x_var)\n",
    "\n",
    "        # find the next sample\n",
    "        x_next = x_next + epsilon * torch.sign(grad_vars[0].data)\n",
    "\n",
    "        # projection\n",
    "        x_next = clip_tensor(x_next)\n",
    "\n",
    "    # rounding step\n",
    "    alpha = torch.rand(x_next.size())\n",
    "    if x_next.is_cuda:\n",
    "        alpha = alpha.cuda()\n",
    "    x_next = round_x(x_next, alpha=alpha)\n",
    "\n",
    "    # feasible projection\n",
    "    x_next = or_float_tensors(x_next, x)\n",
    "\n",
    "    # compute adversarial loss\n",
    "    loss_adv = loss_fct(model(Variable(x_next)), y).data\n",
    "\n",
    "    if is_report_loss_diff:\n",
    "        print(\"Natural loss (%.4f) vs Adversarial loss (%.4f), Difference: (%.4f)\" %\n",
    "              (loss_natural.mean(), loss_adv.mean(), loss_adv.mean() - loss_natural.mean()))\n",
    "\n",
    "    replace_flag = (loss_adv < loss_natural).unsqueeze(1).expand_as(x_next)\n",
    "    x_next[replace_flag] = x[replace_flag]\n",
    "\n",
    "    if x_next.is_cuda:\n",
    "        x_next = x_next.cpu()\n",
    "\n",
    "    return x_next\n",
    "\n",
    "\n",
    "def bga_k(x, y, model, loss_fct, k=25, is_report_loss_diff=False, use_sample=False):\n",
    "    \"\"\"\n",
    "    Multi-step bit gradient ascent\n",
    "    :param x: (tensor) feature vector\n",
    "    :param y:\n",
    "    :param model: nn model\n",
    "    :param loss_fct: loss function\n",
    "    :param k: num of steps\n",
    "    :param is_report_loss_diff:\n",
    "    :param use_sample:\n",
    "    :return: the adversarial version of x according to bga_k (tensor)\n",
    "    \"\"\"\n",
    "    # some book-keeping\n",
    "    sqrt_m = torch.from_numpy(np.sqrt([x.size()[1]])).float()\n",
    "\n",
    "    if next(model.parameters()).is_cuda:\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        sqrt_m = sqrt_m.cuda()\n",
    "\n",
    "    y = Variable(y)\n",
    "\n",
    "    # compute natural loss\n",
    "    loss_natural = loss_fct(model(Variable(x)), y).data\n",
    "\n",
    "    # keeping worst loss\n",
    "    loss_worst = loss_natural.clone()\n",
    "    x_worst = x.clone()\n",
    "\n",
    "    # multi-step with gradients\n",
    "    loss = None\n",
    "    x_var = None\n",
    "    x_next = None\n",
    "    for t in range(k):\n",
    "        if t == 0:\n",
    "            # initialize starting point\n",
    "            x_next = get_x0(x, use_sample)\n",
    "        else:\n",
    "            # compute gradient\n",
    "            grad_vars = torch.autograd.grad(loss.mean(), x_var)\n",
    "            grad_data = grad_vars[0].data\n",
    "\n",
    "            # compute the updates\n",
    "            x_update = (sqrt_m * (1. - 2. * x_next) * grad_data >= torch.norm(\n",
    "                grad_data, 2, 1).unsqueeze(1).expand_as(x_next)).float()\n",
    "\n",
    "            # find the next sample with projection to the feasible set\n",
    "            x_next = xor_float_tensors(x_update, x_next)\n",
    "            x_next = or_float_tensors(x_next, x)\n",
    "\n",
    "        # forward pass\n",
    "        x_var = Variable(x_next, requires_grad=True)\n",
    "        y_model = model(x_var)\n",
    "        loss = loss_fct(y_model, y)\n",
    "\n",
    "        # update worst loss and adversarial samples\n",
    "        replace_flag = (loss.data > loss_worst)\n",
    "        loss_worst[replace_flag] = loss.data[replace_flag]\n",
    "        x_worst[replace_flag.unsqueeze(1).expand_as(x_worst)] = x_next[replace_flag.unsqueeze(1)\n",
    "                                                                       .expand_as(x_worst)]\n",
    "\n",
    "    if is_report_loss_diff:\n",
    "        print(\"Natural loss (%.4f) vs Adversarial loss (%.4f), Difference: (%.4f)\" %\n",
    "              (loss_natural.mean(), loss_worst.mean(), loss_worst.mean() - loss_natural.mean()))\n",
    "\n",
    "    if x_worst.is_cuda:\n",
    "        x_worst = x_worst.cpu()\n",
    "\n",
    "    return x_worst\n",
    "\n",
    "\n",
    "def bca_k(x, y, model, loss_fct, k=25, is_report_loss_diff=False, use_sample=False):\n",
    "    \"\"\"\n",
    "    Multi-step bit coordinate ascent\n",
    "    :param use_sample:\n",
    "    :param is_report_loss_diff:\n",
    "    :param y:\n",
    "    :param x: (tensor) feature vector\n",
    "    :param model: nn model\n",
    "    :param loss_fct: loss function\n",
    "    :param k: num of steps\n",
    "    :return: the adversarial version of x according to bca_k (tensor)\n",
    "    \"\"\"\n",
    "    if next(model.parameters()).is_cuda:\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "\n",
    "    y = Variable(y)\n",
    "\n",
    "    # compute natural loss\n",
    "    loss_natural = loss_fct(model(Variable(x)), y).data\n",
    "\n",
    "    # keeping worst loss\n",
    "    loss_worst = loss_natural.clone()\n",
    "    x_worst = x.clone()\n",
    "\n",
    "    # multi-step with gradients\n",
    "    loss = None\n",
    "    x_var = None\n",
    "    x_next = None\n",
    "    for t in range(k):\n",
    "        if t == 0:\n",
    "            # initialize starting point\n",
    "            x_next = get_x0(x, use_sample)\n",
    "        else:\n",
    "            # compute gradient\n",
    "            grad_vars = torch.autograd.grad(loss.mean(), x_var)\n",
    "            grad_data = grad_vars[0].data\n",
    "\n",
    "            # compute the updates (can be made more efficient than this)\n",
    "            aug_grad = (1. - 2. * x_next) * grad_data\n",
    "            val, _ = torch.topk(aug_grad, 1)\n",
    "            x_update = (aug_grad >= val.expand_as(aug_grad)).float()\n",
    "\n",
    "            # find the next sample with projection to the feasible set\n",
    "            x_next = xor_float_tensors(x_update, x_next)\n",
    "            x_next = or_float_tensors(x_next, x)\n",
    "\n",
    "        # forward pass\n",
    "        x_var = Variable(x_next, requires_grad=True)\n",
    "        y_model = model(x_var)\n",
    "        loss = loss_fct(y_model, y)\n",
    "\n",
    "        # update worst loss and adversarial samples\n",
    "        replace_flag = (loss.data > loss_worst)\n",
    "        loss_worst[replace_flag] = loss.data[replace_flag]\n",
    "        x_worst[replace_flag.unsqueeze(1).expand_as(x_worst)] = x_next[replace_flag.unsqueeze(1)\n",
    "                                                                       .expand_as(x_worst)]\n",
    "\n",
    "    if is_report_loss_diff:\n",
    "        print(\"Natural loss (%.4f) vs Adversarial loss (%.4f), Difference: (%.4f)\" %\n",
    "              (loss_natural.mean(), loss_worst.mean(), loss_worst.mean() - loss_natural.mean()))\n",
    "\n",
    "    if x_worst.is_cuda:\n",
    "        x_worst = x_worst.cpu()\n",
    "\n",
    "    return x_worst\n",
    "\n",
    "\n",
    "def grosse_k(x, y, model, loss_fct, k=25, is_report_loss_diff=False, use_sample=False):\n",
    "    \"\"\"\n",
    "    Multi-step bit coordinate ascent using gradient of output, advancing in direction of maximal change\n",
    "    :param use_sample:\n",
    "    :param is_report_loss_diff:\n",
    "    :param loss_fct:\n",
    "    :param y:\n",
    "    :param x: (tensor) feature vector\n",
    "    :param model: nn model\n",
    "    :param k: num of steps\n",
    "    :return adversarial version of x (tensor)\n",
    "    \"\"\"\n",
    "\n",
    "    if next(model.parameters()).is_cuda:\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "\n",
    "    y = Variable(y)\n",
    "\n",
    "    # compute natural loss\n",
    "    loss_natural = loss_fct(model(Variable(x)), y).data\n",
    "\n",
    "    # keeping worst loss\n",
    "    loss_worst = loss_natural.clone()\n",
    "    x_worst = x.clone()\n",
    "\n",
    "    output = None\n",
    "    x_var = None\n",
    "    x_next = None\n",
    "    for t in range(k):\n",
    "        if t == 0:\n",
    "            # initialize starting point\n",
    "            x_next = get_x0(x, use_sample)\n",
    "        else:\n",
    "            grad_vars = torch.autograd.grad(output[:, 0].mean(), x_var)\n",
    "            grad_data = grad_vars[0].data\n",
    "\n",
    "            # Only consider gradients for points of 0 value\n",
    "            aug_grad = (1. - x_next) * grad_data\n",
    "            val, _ = torch.topk(aug_grad, 1)\n",
    "            x_update = (aug_grad >= val.expand_as(aug_grad)).float()\n",
    "\n",
    "            # find the next sample with projection to the feasible set\n",
    "            x_next = xor_float_tensors(x_update, x_next)\n",
    "            x_next = or_float_tensors(x_next, x)\n",
    "\n",
    "        x_var = Variable(x_next, requires_grad=True)\n",
    "        output = model(x_var)\n",
    "\n",
    "        loss = loss_fct(output, y)\n",
    "\n",
    "        # update worst loss and adversarial samples\n",
    "        replace_flag = (loss.data > loss_worst)\n",
    "        loss_worst[replace_flag] = loss.data[replace_flag]\n",
    "        x_worst[replace_flag.unsqueeze(1).expand_as(x_worst)] = x_next[replace_flag.unsqueeze(1)\n",
    "                                                                       .expand_as(x_worst)]\n",
    "\n",
    "    if is_report_loss_diff:\n",
    "        print(\"Natural loss (%.4f) vs Adversarial loss (%.4f), Difference: (%.4f)\" %\n",
    "              (loss_natural.mean(), loss_worst.mean(), loss_worst.mean() - loss_natural.mean()))\n",
    "\n",
    "    if x_worst.is_cuda:\n",
    "        x_worst = x_worst.cpu()\n",
    "\n",
    "    return x_worst\n",
    "\n",
    "def grams(x, y, model, loss_fct, is_sample=False):\n",
    "    \"\"\"\n",
    "    GRAMS - Greedy Random Accelerated Multi-bit Search\n",
    "    :param x: (tensor) feature vector\n",
    "    :param y:\n",
    "    :param model:\n",
    "    :param loss_fct:\n",
    "    :param use_sample:\n",
    "    \"\"\"\n",
    "    if next(model.parameters()).is_cuda:\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "\n",
    "    y = Variable(y)\n",
    "    \n",
    "#     initialization\n",
    "    best_x = x\n",
    "    orig_x = get_x0(x, is_sample)\n",
    "    k = 8\n",
    "    \n",
    "    TOPK = np.zeros(orig_x.size()[1])\n",
    "    \n",
    "    while (k > 0.5):\n",
    "        # forward pass & compute loss\n",
    "        x_var = Variable(x, requires_grad=True)\n",
    "        y_model = model(x_var)\n",
    "        loss = loss_fct(y_model, y)\n",
    "\n",
    "        # compute gradient\n",
    "        grad_vars = torch.autograd.grad(loss.mean(), x_var)\n",
    "        grad_vars = grad_vars[0].data\n",
    "        \n",
    "        # compute sign\n",
    "#         sign_vars = torch.sign(grad_vars)\n",
    "        \n",
    "        # compute gradient\n",
    "        grad_vars = torch.abs(grad_vars - orig_x*grad_vars)\n",
    "        \n",
    "        # compute updated x'\n",
    "        _, indices = torch.topk(grad_vars, int(k))\n",
    "        TOPK[indices.tolist()[0]] = 1 # topk(grad, k) âˆ— sign\n",
    "        TOPK_vars = Variable(torch.FloatTensor(TOPK).cpu())\n",
    "        x_prime = x_var + TOPK_vars\n",
    "        \n",
    "        # compute loss for x'\n",
    "        y_model = model(x_prime)\n",
    "        loss = loss_fct(y_model, y)\n",
    "        \n",
    "        # compute loss' for best_x\n",
    "        x_var = Variable(best_x, requires_grad=True)\n",
    "        y_model = model(x_var)\n",
    "        loss_prime = loss_fct(y_model, y)\n",
    "        \n",
    "        rows = torch.gt(loss, loss_prime).data\n",
    "        if rows.any():\n",
    "            for r in range(len(rows)):\n",
    "                if rows[r] == True:\n",
    "                    best_x[r] = x_prime.data[r]\n",
    "            x = x_prime.data\n",
    "            k = 2 * k\n",
    "        else:\n",
    "            k = 0.5 * k\n",
    "    \n",
    "    if best_x.is_cuda:\n",
    "        best_x = best_x.cpu()\n",
    "    \n",
    "    return best_x\n",
    "\n",
    "def inner_maximizer(x, y, model, loss_fct, iterations=100, method='natural'):\n",
    "    \"\"\"\n",
    "    A wrapper function for the above algorithim\n",
    "    :param iterations:\n",
    "    :param x:\n",
    "    :param y:\n",
    "    :param model:\n",
    "    :param loss_fct:\n",
    "    :param method: one of 'dfgsm_k', 'rfgsm_k', 'bga_k', 'bca_k', 'natural\n",
    "    :return: adversarial examples\n",
    "    \"\"\"\n",
    "    if method == 'dfgsm_k':\n",
    "        return dfgsm_k(x, y, model, loss_fct, k=iterations)\n",
    "    elif method == 'rfgsm_k':\n",
    "        return rfgsm_k(x, y, model, loss_fct, k=iterations)\n",
    "    elif method == 'bga_k':\n",
    "        return bga_k(x, y, model, loss_fct, k=iterations)\n",
    "    elif method == 'bca_k':\n",
    "        return bca_k(x, y, model, loss_fct, k=iterations)\n",
    "    elif method == 'grosse':\n",
    "        return grosse_k(x, y, model, loss_fct, k=iterations)\n",
    "    elif method == 'natural':\n",
    "        return x\n",
    "    elif method == 'grams':\n",
    "        return grams(x, y, model, loss_fct)\n",
    "    else:\n",
    "        raise Exception('No such inner maximizer algorithm')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
